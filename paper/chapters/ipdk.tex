% !TeX root = ../sn.tex
\documentclass[../sn.tex]{subfiles}
\graphicspath{{\subfix{../images/}}}

\begin{document}
\subsection{Introduction}
IPDK is an open-source development framework, community-driven and target-agnostic which runs on CPU, IPU, DPU or switch.
The main maintainer is Intel but also other companies are implied like Marvell, Ericsson, etc.
What IPDK does is create an abstraction level between the target and the implementation to offload and manage infrastructures that run on different targets.
Under the hood, it uses a few well-established linux frameworks like \acrshort{spdk}, \acrshort{dpdk} and \acrshort{p4}.

\acrshort{ipdk} introduces two different standardized interfaces:
\begin{itemize}
    \item \bold{Infrastructure Application Interface}: Developed starting from pre-existing networking interfaces.
    It uses \acrshort{rpc} mechanisms to allow applications to run locally, remotely or a mix of both.
    This interface uses a couple of tools:
        \begin{itemize}
            \item \bold{P4Runtime}: Used for programmable networking data plane
            \item \bold{OpenConfig}: Used for configuring physical ports, virtual devices, QoS and inline operations such as IPsec
        \end{itemize}
    \item \bold{Target Abstraction Interface}: The Target Abstraction Interface (TAI) is an abstraction exposed by an infrastructure device running infrastructure applications that serve connected compute instances (attached hosts and/or VMs, which may or may not be containerized).
    The infrastructure device may also contain an isolated Infrastructure Manager used to help manage the lifecycle of the device.
\end{itemize}
In the image below it's possible to see how the two interfaces work and in which kind of communication they are involved\cite{ipdk-introduction}.
\centeredimage{ipdk-layers}{IPDK layers and interfaces interactions}
Using \acrshort{ipdk} it is possible to build either Virtual Networking or Virtual Storage environments.

\subsubsection{Virtual Networking}
When we talk about \acrshort{vn} in the context of \acrshort{ipdk} we are referring to a single virtual device type which is \emph{virtio-net}.
Is it possible to create a \acrlong{vn} in a bare metal server that hosts \acrshort{vm}s or in a nested virtualized environment, in both cases the configuration is the same.
\centeredimage{vn-iaas}{Virtual Networking Example}
In the image above we can see 4 different steps needed to hotplug a new virtual device, in this case, \acrshort{sw} Target
\begin{itemize}
    \item \bold{1.} Over OpenConfig, create a new virtual port (specifying the type, number of queues, etc.).
    \item \bold{2.} This new virtual port is associated with a netdev in P4 OVS, and a corresponding port device is hotplugged into the host or VM.
    Any exception traffic from that hotplug'd device will arrive on this netdev.
    Any traffic P4 OVS wants to direct to this device can be sent over this netdev as well.
    \item \bold{3.} The tenant instance loads the corresponding driver for that device and can now send/receive traffic.
    \item \bold{4.} The virtual private cloud data-plane moves traffic to/from the instance, sending exceptions to the infrastructure software when needed\cite{ipdk-virtual-networking}.
\end{itemize}

\subsubsection{Virtual Storage}
In this case, there are 3 different devices supported by the suite:
\begin{itemize}
    \item virtio-blk
    \item virtio-scsi
    \item NVMe
\end{itemize}
Using the same schema that we've seen in the \acrshort{vn} part we can describe how the \acrlong{vs} management is achieved
\centeredimage{vs-iaas}{Virtual Storage Example}
\begin{itemize}
    \item \bold{1.} Over OpenConfig, create a new virtual storage controller device, specifying the device type, number of queues, etc.
    \item \bold{2.} The storage controller has one or more attached volumes.
    The maximum number of volumes is dependent on the device type.
    Each volume is associated with an SPDK block device (bdev) that implements the actual storage backing the volume.
    \item \bold{3.} The tenant instance loads the corresponding driver for that device, and can then send storage requests to volumes attached to the storage controller that will be processed by the SPDK storage application.
    \item \bold{4.} Storage requests (reads, writes, flushes) directed to a volume will arrive at the SPDK target layer and be translated and forwarded to its associated SPDK bdev.
    SPDK bdevs can be of many different types, depending on the type of backing storage for the volume.
    Examples include NVMe over TCP, NVMe over RDMA, or Ceph RBD.
    \item \bold{5.} The SPDK bdev module responsible for a given bdev type forwards the storage request to the bdev's associated remote target.
    For example, the NVMe/TCP bdev module is responsible for establishing TCP connections to the remote NVMe/TCP target and sending NVMe CONNECT commands to associate those connections with the NVMe subsystem containing the volume associated with the tenant's virtual storage controller.
    It also constructs NVMe commands for the storage request(s) and sends them (along with any write data) to the remote target over the previously established TCP socket.
\end{itemize}
\acrshort{ipdk} provides a good number of recipes where to start to develop a custom solution.
Almost all the solutions are based on P4-OVS and a P4 program.

\subsection{Build, Deployment and Management}
The most interesting thing about \acrshort{ipdk} is that it introduces some management components that are dynamically created during the build process based on the P4 program. To better understand what happens under the hood let's see some main commands provided by \acrshort{ipdk} that generate the management layer.

\subsubsection*{P4 Compile}
\centeredimage{compile}{P4 compile command}
the command above compiles the P4 program and it creates 3 different files: \bold{bf-rt.json}, \bold{context.json} and \bold{p4info.txt} which are used in the next command to generate a management layer.

\subsubsection*{Build Pipeline Management Layer}
\centeredimage{pipeline_builder}{Generate a management layer based on the compilation output}
the command above takes \bold{.conf} file which contains the locations of the 3 files generated by the previous command. This command combines the artifacts generated by p4c compiler to generate a single bin file (*.pb.bin) to be pushed to the target.

\subsubsection*{Management}
When a deployment is performed by IPDK we will find a management layer that helps us to modify the target configuration. This management layer is a gRPC server running on the target that can be queried by some CLIs such as:
\begin{itemize}
    \item \bold{ovs-p4rt}: A library (C++ with a C interface) that allows ovs-vswitchd and ovsdb-server to communicate with the P4Runtime Server in infrap4d via gRPC. It is used to program (insert/modify/delete) P4 forwarding tables in the pipeline.
    \item \bold{p4rt-ctl}: A Python-based P4Runtime client which talks to the P4Runtime Server in infrap4d via gRPC, to program the P4 pipeline and insert/delete P4 table entries.
    \item \bold{gnmi\_cli}: A gRPC-based C++ network management interface client to handle port configurations and program fixed functions in the P4 pipeline.
\end{itemize}

Those clients above communicate to a runtime deployed by IPDK into the target. The main component is called infrap4d described as follows:
\subsubsection{Infrap4d}
Infrap4d integrates Stratum, the Kernel Monitor (krnlmon), Switch Abstraction Interface (SAI), Table Driven Interface (TDI), and a P4 target driver into a separate process (daemon).
\centeredimage{infrap4d}{Infrap4d process schema}
In the previous image the target used as example is P4-OVS.
Infrap4d provides 2 interfaces:
\begin{itemize}
    \item \bold{P4Runtime}: The P4Runtime API is a control plane specification for managing the data plane elements of a device defined or described by a P4 program.
    \item \bold{gNMI}: gRPC Network Management Interface (gNMI) is a gRPC-based protocol to manage network devices.
\end{itemize}
\subsubsection*{Stratum}
Stratum is an open-source silicon-independent switch operating system. It is a component of Infrap4d that provides the P4Runtime and gNMI/Openconfig capabilities for P4 flow rule offloads and port configuration offloads. Stratum is augmented with a new tdi platform layer that processes P4rt and gNMI requests and interacts with the underlying P4 target driver through TDI. A new ipdk platform layer provides IPDK-specific replacements for several TDI modules that allow it to handle configuration differences between IPUs and the switches for which Stratum was developed.
\subsubsection*{TDI - Table Driven Interface}
TDI (Table Driven Interface) provides a target-agnostic interface to the driver for a P4-programmable device. It is a set of APIs that enable configuration and management of P4 programmable and fixed functions of a backend device in a uniform and dynamic way. Different targets like bmv2 and P4-DPDK can choose to implement their own backends for different P4 and non-P4 objects but can share a common TDI. Stratum talks to the target-specific driver through the TDI front-end interface.
\subsubsection*{krnlmon - Kernel Monitor}
The Kernel Monitor receives RFC 3549 messages from the Linux Kernel over a Netlink socket when changes are made to the kernel networking data structures. It listens for network events (link, address, neighbor, route, tunnel, etc.) and issues calls to update the P4 tables via SAI and TDI. The kernel monitor is an optional component of infrap4d.
\subsubsection*{SAI - Switch Abstraction Interface}
Switch Abstraction Interface (SAI) defines a vendor-independent interface for switching ASICs.

\subsection{L3 Forwarding Example}
\subsubsection*{Prepare the environment}
Using a new folder called SDE as root folder, assuming /root/SDE for the rest of the example, run the following commands
\centeredimage{git_clone}{Cloning the DPDK target}
\centeredimage{create_install_folder}{Create an install folder to store the executable, deps and libs}
\centeredimage{source_envs}{Source envs}
\centeredimage{install_deps}{Install dependencies that will be stored in /root/SDE/install}
\centeredimage{install_p4_dpdk}{Install P4 DPDK Target}

Now P4 DPDK Target is installed and ready to be configured so we can install the dependencies and build the P4 program.
Let's clone the \emph{Networking Recipe} from \emph{https://github.com/ipdk-io/networking-recipe}. Install some dependencies by running \emph{apt install libatomic1 libnl-route-3-dev} and \emph{pip3 install -r requirements.txt} from inside the networking-recipe folder.

\subsubsection*{Install Infrap4d}
Run all the following commands from inside the networking-recipe folder.
\centeredimage{install_infrap4d}{Install Infrap4d}

Export 2 variables:
\begin{itemize}
    \item DEPEND\_INSTALL=folder where to store deps, i.e. /root/deps
    \item SDE\_INSTALL=/root/SDE/install
\end{itemize}
After being sure that the exported variables are correct run the following command
\centeredimage{source_setup_env}{Exports envs required for the setup proces}

Now we can finally compile the network recipe. This step may take a lot of time because involves source compilation, components building and downloading:
\centeredimage{recipe_compile}{Compile the recipe}

\subsubsection*{Run Infrap4d}
The following commands will run the Infrap4d daemon, run it from inside networking-recipe folder.
\centeredimage{setup_and_run_infrap4d}{Enable hugepages and run Infrap4d}

\subsubsection*{Run Network Recipe}
Create 2 ports using the gnmi-ctl, run it from inside networking-recipe folder:
\centeredimage{create_ports}{Create 2 different TAP ports}
clone the P4 program https://github.com/ipdk-io/ipdk.git. Compile the P4 example program:
\centeredimage{compile}{Compile the P4 Program}
\centeredimage{pipeline_builder}{Build the pipeline}
Now that the pipeline is ready we can set it as current pipeline of the p4runtime which is running in the infrap4d process:
\centeredimage{set-pipe}{Set the pipeline}
We can add an arbitrary number of rules to this pipe but for the moment let's add two rules:
\centeredimage{pipe-rules}{Set pipe's rules}
In the commands above we have a bunch of parameters explained below:
\begin{itemize}
    \item \bold{br0}: the name of the vSwitch assigned internally by p4runtime
    \item \bold{ingress.ipv4\_host}: the name of the table in \emph{dot notation} where the rule should be added
    \item \bold{hdr.ipv4.dst\_addr=<ip>,action=<action>(<params>)}: is a string which explains what the entry should do.
    You should read it as following: when the destination ip of the incoming packet, which is the key of the table, is equal to <ip> perform the action called <action> having <params> as parameters.
\end{itemize}
To test the solution we can simply use a python library called Scapy. To use it run Python3 in a terminal session, and import everything of scapy \bold{\emph{from scapy.all import *}} and send a packet with the following instruction:
\centeredimage{python_send}{Send packet using python CLI}
You can use TCPDUMP to sniff packets on the TAP1 interface and see that the packet has been forwarded correctly. You can try to remove the forwarding rule and you'll see that the packets are not forwarded anymore from TAP0 to TAP1. To remove the rule run 
\begin{center}
    ./install/bin/p4rt-ctl del-entry br0 ingress.ipv4\_host "hdr.ipv4.dst\_addr=2.2.2.2"
\end{center}
\clearpage
\end{document}